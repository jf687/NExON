---
title: "<span style='font-size:36px; color:#2C3E50; font-weight:bold;'>NExON-Bayes</span>"
author: "<span style='font-size:18px;'>A brief guide on how to use NExON Bayes to perform joint network inference</span>"
date: "<span style='font-size:14px;'>August 6th, 2025</span>"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r install_package, include= FALSE}
#remotes::install_github("https://github.com/jf687/NExON")
```

## Simulating Data

For this example, we simulate 4 networks and assign covariate values $a \in \mathcal{A} =  \{1,2,3,4\}$.

```{r covariate_initialisation, echo=TRUE}
A <- 1:4

```

We now generate four corresponding precision matrices with P = 50 variables. To do this, we use our function `Create_Cn_networks()` which implements an algorithm that generates symmetric positive definite matrices, where a fraction of entries are (linearly) dependent on the covariate ordinal value that corresponds to each matrix. We set the `frac_change` variable = 0.4, which means 40% of present edges will decrease with covariate and the same amount of non-edges will increase with covariate. Seeds can be set for reproducibility of both the precision matrices and the generated data.

```{r networks, echo=TRUE}

nets <- NExON::create_Cn_networks(P = 50, Cn = 4, seed_1 = 252, seed_2 = 123, frac_change = 0.4, Ns_sample = c(150))
```
The function also generates normally distributed data from the resulting (inverse) precision matrices, labelled `Ys`. In this example, there are N = 150 samples drawn for each of the four precision matrices.
```{r data, echo = TRUE}
Ys <- nets$Ys

```

## Finding the optimal $\nu_0$ set
Now that we have our true precision matrices and our simulated data, we begin to perform joint graphical estimation using NExON-Bayes. Firstly, values for each networks optimal $\nu_0$ must be found. To do this, we initialise an empty list (`v0_list`) and then use our function `select_optimal_v0_t()` which performs a line search over a range of $\nu_0$ values and returns the value that maximises the extended Bayesian Information Criteria (eBIC) in the vanilla single network estimation case. The tuning parameter, $\gamma$ is set to 0.35. 

```{r v0, echo = TRUE, results="hide", warning=FALSE, fig.show= "none"}
v0_list <- list()

# Select optimal v0s
for(c in 1:length(A)){
  v0_list[[c]] <- NExON::select_optimal_v0_t(Ys[[c]], gamma = 0.35, plot_ = FALSE)$optimal_v0
}
```
## Performing Inference
We now have data (`Ys`), corresponding ordinal covariate values (`A`) and a list of $\nu_0$ values (`v0_list`), so can call the function `NExON()` to perform graphical estimation. We set the standard threshold of 0.5 on the posterior probabilities of inclusion (PPIs), which are labelled `m_delta`. This means that any entries with a corresponding PPI of less than 50% are set to zero:

```{r inference, echo = TRUE, results="hide", warning=FALSE, fig.show= "none"}
out <- NExON::NExON(Ys, A, v0_list = v0_list, debug = T)
out_nets<- lapply(out$estimates$m_deltas, function(x) abs(x)> 0.5)

```

## Assessing Performance
With our results, we can calculate the precision and recall of the estimates, along with a confusion matrix:

```{r performance, echo=TRUE, fig.show='hold'}
eval_nets <- NExON::evaluate_network_list(nets$As, out_nets,0.5)
conf.mat <- eval_nets$conf.mat
cat("Confusion Matrix:\n", conf.mat[1:2], "\n",conf.mat[3:4], "\n")
cat("Precision achieved:\n",NExON::precision(conf.mat),"\n")
cat("Recall achieved:\n",NExON::recall(conf.mat),"\n")
```
## Plotting Results
We can also plot the estimated networks:

```{r est_graphs, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
layout <- NExON::create_layout(nets$As[[1]])
NExON::create_network_plots(out_nets, layout_coords = layout, title = "Estimated Networks")

```

And compare to the true networks:

```{r true_graphs, echo=TRUE, fig.show='hold', results = "hide" }
NExON::create_network_plots(nets$As, layout_coords = layout,
                     color = "blue", edge_color = "green", title = "True Networks")
```

To check the convergence, we plot the ELBO at the M-Step and total ELBO at each iteration:

```{r ELBO, echo=TRUE, fig.show='hold'}
par(mfrow = c(1,2))
plot(out$debugs$vec_ELBO_CM, xlab = "iterations", ylab = "ELBOs at M-step")
plot(unlist(out$debugs$list_ELBO), xlab = "iterations", ylab = "ELBOs")
```

Finally, we can plot the distributions of $\beta_{ij}$ and $\zeta_{ij}$:

```{r beta_zeta, echo=TRUE, fig.show='hold', warning = FALSE}
par(mfrow = c(1,2))

data <- out$estimates$mu_beta[upper.tri(out$estimates$mu_beta)]
data_frame <- data.frame(data)

ggplot(data = data_frame, aes(x = data)) +
  geom_histogram(binwidth = 0.01, color = "black", fill = "skyblue", alpha = 0.7) +
  stat_bin(binwidth = 0.01, geom = "text", aes(label = ..count..), vjust = -0.5) +
  labs(x = "(a) beta", y = "Frequency", title = "") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    panel.grid.major = element_line(size = 0.5, linetype = 'dashed', color = 'gray'),
    panel.grid.minor = element_line(size = 0.25, linetype = 'dashed', color = 'gray')
  )

data <- out$estimates$mu_zeta[upper.tri(out$estimates$mu_zeta)]
data_frame <- data.frame(data)

ggplot(data = data_frame, aes(x = data)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "navyblue", alpha = 0.7) +
  stat_bin(binwidth = 0.5, geom = "text", aes(label = ..count..), vjust = -0.25) +
  labs(x = "(b) zeta", y = "Frequency", title = "") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    panel.grid.major = element_line(size = 0.5, linetype = 'dashed', color = 'gray'),
    panel.grid.minor = element_line(size = 0.25, linetype = 'dashed', color = 'gray')
  )

```


